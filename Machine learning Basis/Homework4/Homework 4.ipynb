{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>0.197905</td>\n",
       "      <td>-0.702212</td>\n",
       "      <td>-0.741774</td>\n",
       "      <td>-0.639366</td>\n",
       "      <td>-0.555608</td>\n",
       "      <td>-0.698853</td>\n",
       "      <td>-0.181827</td>\n",
       "      <td>-0.612927</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>0.197905</td>\n",
       "      <td>0.277252</td>\n",
       "      <td>0.262783</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>1.695166</td>\n",
       "      <td>1.772867</td>\n",
       "      <td>-0.181827</td>\n",
       "      <td>-0.285105</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>-0.511643</td>\n",
       "      <td>-0.702212</td>\n",
       "      <td>-0.741774</td>\n",
       "      <td>-0.639366</td>\n",
       "      <td>-0.555608</td>\n",
       "      <td>-0.424217</td>\n",
       "      <td>-0.181827</td>\n",
       "      <td>-0.612927</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>0.552679</td>\n",
       "      <td>1.583204</td>\n",
       "      <td>1.602192</td>\n",
       "      <td>-0.639366</td>\n",
       "      <td>-0.105454</td>\n",
       "      <td>0.125054</td>\n",
       "      <td>-0.181827</td>\n",
       "      <td>1.354008</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>-0.156869</td>\n",
       "      <td>-0.702212</td>\n",
       "      <td>-0.741774</td>\n",
       "      <td>0.059333</td>\n",
       "      <td>-0.555608</td>\n",
       "      <td>-0.698853</td>\n",
       "      <td>-0.181827</td>\n",
       "      <td>-0.612927</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>776715</td>\n",
       "      <td>-0.511643</td>\n",
       "      <td>-0.702212</td>\n",
       "      <td>-0.741774</td>\n",
       "      <td>-0.639366</td>\n",
       "      <td>-0.105454</td>\n",
       "      <td>-0.424217</td>\n",
       "      <td>-0.998853</td>\n",
       "      <td>-0.612927</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>841769</td>\n",
       "      <td>-0.866417</td>\n",
       "      <td>-0.702212</td>\n",
       "      <td>-0.741774</td>\n",
       "      <td>-0.639366</td>\n",
       "      <td>-0.555608</td>\n",
       "      <td>-0.698853</td>\n",
       "      <td>-0.998853</td>\n",
       "      <td>-0.612927</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>888820</td>\n",
       "      <td>0.197905</td>\n",
       "      <td>2.236180</td>\n",
       "      <td>2.271896</td>\n",
       "      <td>0.059333</td>\n",
       "      <td>1.695166</td>\n",
       "      <td>-0.149582</td>\n",
       "      <td>1.860738</td>\n",
       "      <td>2.337476</td>\n",
       "      <td>0.229166</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>897471</td>\n",
       "      <td>-0.156869</td>\n",
       "      <td>1.583204</td>\n",
       "      <td>0.932487</td>\n",
       "      <td>0.408682</td>\n",
       "      <td>-0.105454</td>\n",
       "      <td>0.125054</td>\n",
       "      <td>2.677764</td>\n",
       "      <td>1.026185</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>897471</td>\n",
       "      <td>-0.156869</td>\n",
       "      <td>1.583204</td>\n",
       "      <td>1.602192</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>0.344701</td>\n",
       "      <td>0.399689</td>\n",
       "      <td>2.677764</td>\n",
       "      <td>0.370540</td>\n",
       "      <td>-0.348400</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "0               1000025         0.197905                -0.702212   \n",
       "1               1002945         0.197905                 0.277252   \n",
       "2               1015425        -0.511643                -0.702212   \n",
       "3               1016277         0.552679                 1.583204   \n",
       "4               1017023        -0.156869                -0.702212   \n",
       "..                  ...              ...                      ...   \n",
       "694              776715        -0.511643                -0.702212   \n",
       "695              841769        -0.866417                -0.702212   \n",
       "696              888820         0.197905                 2.236180   \n",
       "697              897471        -0.156869                 1.583204   \n",
       "698              897471        -0.156869                 1.583204   \n",
       "\n",
       "     Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "0                   -0.741774          -0.639366                    -0.555608   \n",
       "1                    0.262783           0.758032                     1.695166   \n",
       "2                   -0.741774          -0.639366                    -0.555608   \n",
       "3                    1.602192          -0.639366                    -0.105454   \n",
       "4                   -0.741774           0.059333                    -0.555608   \n",
       "..                        ...                ...                          ...   \n",
       "694                 -0.741774          -0.639366                    -0.105454   \n",
       "695                 -0.741774          -0.639366                    -0.555608   \n",
       "696                  2.271896           0.059333                     1.695166   \n",
       "697                  0.932487           0.408682                    -0.105454   \n",
       "698                  1.602192           0.758032                     0.344701   \n",
       "\n",
       "     Bare Nuclei  Bland Chromatin  Normal Nucleoli   Mitoses  Class  \n",
       "0      -0.698853        -0.181827        -0.612927 -0.348400      2  \n",
       "1       1.772867        -0.181827        -0.285105 -0.348400      2  \n",
       "2      -0.424217        -0.181827        -0.612927 -0.348400      2  \n",
       "3       0.125054        -0.181827         1.354008 -0.348400      2  \n",
       "4      -0.698853        -0.181827        -0.612927 -0.348400      2  \n",
       "..           ...              ...              ...       ...    ...  \n",
       "694    -0.424217        -0.998853        -0.612927 -0.348400      2  \n",
       "695    -0.698853        -0.998853        -0.612927 -0.348400      2  \n",
       "696    -0.149582         1.860738         2.337476  0.229166      4  \n",
       "697     0.125054         2.677764         1.026185 -0.348400      4  \n",
       "698     0.399689         2.677764         0.370540 -0.348400      4  \n",
       "\n",
       "[683 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 首先将缺失值中？表示替换为nan\n",
    "data = pd.read_csv(\"breast-cancer-wisconsin.data\",names=[\"Sample code number\",\"Clump Thickness\",\"Uniformity of Cell Size\",\"Uniformity of Cell Shape\",\"Marginal Adhesion\",\"Single Epithelial Cell Size\",\"Bare Nuclei\",\"Bland Chromatin\",\"Normal Nucleoli\",\"Mitoses\",\"Class\"])\n",
    "data.replace(\"?\", np.nan , inplace = True)\n",
    "data[\"Bare Nuclei\"] = data[\"Bare Nuclei\"].astype(\"float\")\n",
    "cleaned_data = data.dropna()\n",
    "normalized_data = pd.DataFrame()\n",
    "def Standard_Score(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return ((arr - mean)/std)\n",
    "for (columnName, columnData) in cleaned_data.iteritems():\n",
    "    normalized_data[columnName] = Standard_Score(columnData)\n",
    "normalized_data[\"Sample code number\"] = cleaned_data[\"Sample code number\"]\n",
    "normalized_data[\"Class\"] = cleaned_data[\"Class\"]\n",
    "normalized_data.to_csv(\"normalized-breast-cancer-wisconsin.data\",header=False,index=False,sep=',')\n",
    "display(normalized_data)\n",
    "\n",
    "X = normalized_data.iloc[:,1:-1]\n",
    "#Class 一列中，4表明患癌症，2表示不换癌症。转化成1-0\n",
    "y = normalized_data[\"Class\"].replace([4,2],[1,0]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : 分割训练集和测试集\n",
    "逻辑回归通过定义$\\beta = \\begin{bmatrix}w\\\\b \\end{bmatrix},\\hat{\\mathbf{x}}=\\begin{bmatrix}\\mathbf{x}\\\\\\mathbb{1}\\end{bmatrix}$ ，将 $\\mathbf{ \\omega}^\\top \\mathbf{x} +{ b}$ 简写为 $\\mathbf{\\beta}^\\top\\hat{\\mathbf{x}}$, 所以通过给$\\mathbf{X}$加一列全1向量变成增广矩阵$\\hat{\\mathbf{X}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = np.concatenate((X.values,np.ones([X.shape[0],1])),axis = 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=20)\n",
    "X_hat_train = np.concatenate((X_train.values,np.ones([X_train.shape[0],1])),axis = 1)\n",
    "X_hat_test = np.concatenate((X_test.values,np.ones([X_test.shape[0],1])),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : 梯度下降法估计Logistic Regression模型的参数$\\beta$\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(-y_{i} \\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}+\\ln \\left(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}}\\right)\\right)\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{*}=\\underset{\\boldsymbol{\\beta}}{\\arg \\min } \\ell(\\boldsymbol{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于梯度下降法而言：\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{t+1}=\\boldsymbol{\\beta}^{t}- s \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}\\\\\n",
    "\n",
    "\\nabla \\ell =\\hat{\\mathbf{X}}^{\\top}(\\boldsymbol{\\mu}-\\mathbf{y})\n",
    "$$\n",
    "### 其中：\n",
    "$$\\boldsymbol{\\mu}=\\left(\\mu_{1}, \\ldots, \\mu_{n}\\right)^{\\mathrm{T}}$$\n",
    "$$\n",
    "\\mu_{i}=\\frac{1}{1+\\exp \\left(-\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\mathbf{x}_{i}}\\right)}, i=1, \\ldots, n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用梯度下降法迭代次数为:358 \n",
      "目标函数最优值为：30.526983644244435 \n",
      "最优解为：[ 1.24668391  1.61365038  1.55144388  0.50037237  0.29338002  1.73905281\n",
      "  0.72092771  0.44772339  0.72605074 -0.71245936]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#梯度下降法\n",
    "def target_function(X_hat,beta,y):\n",
    "    result = 0\n",
    "    for i in range(X_hat.shape[0]):\n",
    "        result += -y[i]* beta.T @ X_hat[i,:] + np.log( 1+ np.exp(beta.T @ X_hat[i,:]))\n",
    "    return result\n",
    "\n",
    "def mu(X_hat,beta,y):\n",
    "    mu = []\n",
    "    for i in range(X_hat.shape[0]):\n",
    "        mu.append(float(1/(1+np.exp(-beta.T @ X_hat[i,:]))))\n",
    "    mu = np.array(mu)\n",
    "    return mu\n",
    "\n",
    "def gradient(X_hat,beta,y):\n",
    "    m = mu(X_hat,beta,y)\n",
    "    return X_hat.T @ (m - y)\n",
    "\n",
    "def Gradient_Decent(X_hat,y,step,eps):\n",
    "    beta = np.zeros(X_hat.shape[1])\n",
    "    t=0 #计数器 \n",
    "    err = np.inf\n",
    "    while err > eps and t < 1e6:\n",
    "        original_lx = target_function(X_hat,beta,y)\n",
    "        beta=beta-step*gradient(X_hat,beta,y)\n",
    "        err = abs(target_function(X_hat,beta,y) - original_lx)\n",
    "        t += 1\n",
    "    return beta,t,target_function(X_hat,beta,y)\n",
    "GD_beta,GD_t,GD_target = Gradient_Decent(X_hat_train , y_train , step = 1e-2, eps = 1e-6)\n",
    "print( \"使用梯度下降法迭代次数为:{t} \\n目标函数最优值为：{target} \\n最优解为：{beta}\\n\".format(t = GD_t,target = GD_target,beta = GD_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : 牛顿法估计Logistic Regression模型的参数$\\beta$\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(-y_{i} \\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}+\\ln \\left(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}}\\right)\\right)\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{*}=\\underset{\\boldsymbol{\\beta}}{\\arg \\min } \\ell(\\boldsymbol{\\beta})\n",
    "$$\n",
    "### 对于牛顿法而言：\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{t+1}=\\boldsymbol{\\beta}^{t}-\\left(\\frac{\\partial^{2} \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^{\\mathrm{T}}}\\right)^{-1} \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}=\\boldsymbol{\\beta}^{t} - \\mathbf{H} \\nabla \\ell\n",
    "$$\n",
    "### 其中：\n",
    "$$\\boldsymbol{\\mu}=\\left(\\mu_{1}, \\ldots, \\mu_{n}\\right)^{\\mathrm{T}}$$\n",
    "$$\n",
    "\\mu_{i}=\\frac{1}{1+\\exp \\left(-\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\mathbf{x}_{i}}\\right)}, i=1, \\ldots, n$$\n",
    "$$\\nabla \\ell=\\hat{\\mathbf{X}}^{\\mathrm{T}}(\\boldsymbol{\\mu}-\\mathbf{y})$$\n",
    "$$\\mathbf{H}=\\hat{\\mathbf{X}}^{\\mathrm{T}} \\mathbf{S} \\hat{\\mathbf{X}}$$\n",
    "$$\\mathbf{S}=\\operatorname{diag}\\left(\\mu_{1}\\left(1-\\mu_{1}\\right), \\ldots, \\mu_{n}\\left(1-\\mu_{n}\\right)\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用牛顿法迭代次数为:9 \n",
      "目标函数最优值为：30.526898976202812 \n",
      "最优解为：[ 1.24547846  1.60078209  1.56240374  0.50129893  0.29353511  1.73881993\n",
      "  0.72137369  0.44653756  0.72704434 -0.71371476]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Hessian(X_hat,beta,y):\n",
    "    S = np.zeros([X_hat.shape[0],X_hat.shape[0]])\n",
    "    m = mu(X_hat,beta,y)\n",
    "    for i in range(X_hat.shape[0]):\n",
    "        S[i,i] = m[i]*(1-m[i])\n",
    "    return X_hat.T @ S @ X_hat\n",
    "\n",
    "def Newton_Method(X_hat,y,eps):\n",
    "    beta = np.zeros(X_hat.shape[1])\n",
    "    t=0 #计数器 \n",
    "    err = np.inf\n",
    "    while err > eps and t < 1e6:\n",
    "        original_lx = target_function(X_hat,beta,y)\n",
    "        beta=beta - np.linalg.inv(Hessian(X_hat,beta,y)) @ gradient(X_hat,beta,y)\n",
    "        err = abs(target_function(X_hat,beta,y) - original_lx)\n",
    "        t += 1\n",
    "    return beta,t,target_function(X_hat,beta,y)\n",
    "\n",
    "newton_beta,newton_t,newton_target = Newton_Method(X_hat_train , y_train , eps = 1e-6)\n",
    "print( \"使用牛顿法迭代次数为:{t} \\n目标函数最优值为：{target} \\n最优解为：{beta}\\n\".format(t = newton_t,target = newton_target,beta = newton_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 : Sklearn 求解 Logistic Regression 模型的参数 $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标函数最优值为：56.26488479882792 \n",
      "使用sklearn模型得到的参数beta为：[ 1.09769359  1.15232508  1.2333593   0.51589683  0.40528952  1.46062189\n",
      "  0.69647669  0.48455336  0.60369775 -0.78794857]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression().fit(X_train,y_train)\n",
    "sklearn_beta = np.concatenate((logistic_regression.coef_.flatten(),logistic_regression.intercept_))\n",
    "print(\"目标函数最优值为：{target} \\n使用sklearn模型得到的参数beta为：{beta}\\n\".format(target = target_function(X_hat,sklearn_beta,y),beta = sklearn_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6 : 对比各种回归方式异同\n",
    "将梯度下降法和牛顿法得到的系数放入sklearn对象中，方便使用sklearn中函数进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_decent = LogisticRegression().fit(X_train,y_train)\n",
    "gradient_decent.coef_ = GD_beta[:-1].reshape(1,GD_beta.shape[0]-1)\n",
    "gradient_decent.intercept_ = GD_beta[-1]\n",
    "newton = LogisticRegression().fit(X_train,y_train)\n",
    "newton.coef_ = newton_beta[:-1].reshape(1,newton_beta.shape[0]-1)\n",
    "newton.intercept_ = newton_beta[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比回归系数 $ \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD方法系数：[ 1.24668391  1.61365038  1.55144388  0.50037237  0.29338002  1.73905281\n",
      "  0.72092771  0.44772339  0.72605074 -0.71245936]\n",
      "Newton方法系数：[ 1.24547846  1.60078209  1.56240374  0.50129893  0.29353511  1.73881993\n",
      "  0.72137369  0.44653756  0.72704434 -0.71371476]\n",
      "Sklearn方法系数：[ 1.09769359  1.15232508  1.2333593   0.51589683  0.40528952  1.46062189\n",
      "  0.69647669  0.48455336  0.60369775 -0.78794857]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GD方法系数：{gd}\\nNewton方法系数：{nt}\\nSklearn方法系数：{sk}\\n\".format(gd = GD_beta,nt = newton_beta,sk = sklearn_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD方法准确率：0.9761904761904762\n",
      "Newton法准确率：0.9761904761904762\n",
      "Sklearn方法准确率：0.978021978021978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GD_score = gradient_decent.score(X_train,y_train) #由于eps设置的小，所以测试集算出准确率相同\n",
    "newton_score = newton.score(X_train,y_train)\n",
    "sk_score = logistic_regression.score(X_train,y_train)\n",
    "print(\"GD方法准确率：{gd}\\nNewton法准确率：{nt}\\nSklearn方法准确率：{sk}\\n\".format(gd = GD_score,nt = newton_score,sk = sk_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论\n",
    "由上述结论可知，模型跟迭代次数有一定关系:\n",
    "\n",
    "牛顿法的目标函数值是最小的，效果是最好的。\n",
    "\n",
    "sklearn的目标函数值是最大的，效果最差。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
