{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五次作业\n",
    "## 1. 推导概率潜在语义分析的共现模型的EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "潜在语义分析的共现模型定义如下：\n",
    "\n",
    "因为共现模型假设在话题$z$给定的情况下，单词$w$与文本$d$是条件独立的，所以每个单词-文本对 $(w, d)$ 的概率由以下公式决定:\n",
    "\n",
    "$$\n",
    "P(w, d)=\\sum_{z \\in Z} P(z) P(w \\mid z) P(d \\mid z)\n",
    "$$\n",
    "\n",
    "文本-单词共现数据 $T$ 的生成概率为所有单词-文本对 $(w, d)$ 的生成概率的乘积:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L = P(T)&=\\prod_{(w, d)} P(w, d)^{n(w, d)}\\\\\n",
    "&=\\prod_{i=1}^M \\prod_{j=1}^N P(w_i, d_j)^{n(w_i, d_j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对似然函数取对数后得：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL &=\\sum_{i=1}^M \\sum_{j=1}^N {n(w_i, d_j)} \\log(P(w_i, d_j))\\\\\n",
    "&= \\sum_{i=1}^{M} \\sum_{j=1}^{N} n(w_i, d_j) \\log \\left(\\sum_{k=1}^{K} P(z_k) \\frac{P(w_i, d_j,z_k)}{P(z_k)}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$n(w_i, d_i)$表示$(w_i, d_i)$出现的次数。\n",
    "\n",
    "根据Jesen不等式：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL &= \\sum_{i=1}^{M} \\sum_{j=1}^{N} n(w_i, d_j) \\log \\left(\\sum_{k=1}^{K} P(z_k) \\frac{P(w_i, d_j,z_k)}{P(z_k)}\\right) \\\\\n",
    "&\\geq \\sum_{i=1}^{M} \\sum_{j=1}^{N} n(w_i, d_j) \\left(\\sum_{k=1}^{K} P(z_k) \\log \\frac{P(w_i, d_j,z_k)}{P(z_k)}\\right) \\\\\n",
    "&=\\sum_{i=1}^{M} \\sum_{j=1}^{N} \\sum_{k=1}^{K} n(w_i, d_j) \\left(P(z_k) \\log \\left(P(w_i, d_j,z_k)\\right)-P(z_k)\\log\\left(P(z_k)\\right)\\right) \\\\\n",
    "&\\triangleq J(\\theta,P(z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**E（expectation）步：**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P^{(t)}(z_k)&= \\argmax_{P(z_k)} J(w^{(t)},d^{(t)},P(z_k))\\\\\n",
    "&=P(z_k \\mid w_i^{(t)}, d_j^{(t)})\\\\\n",
    "&= \\frac{P(w_i^{(t)}, d_j^{(t)}, z_k)}{P(w_i^{(t)}, d_j^{(t)})} \\\\ \n",
    "&= \\frac{P(z_k)P(w_i^{(t)}|z_k)P(d_j^{(t)}|z_k)}{\\sum_{k=1}^{K} P(z_k)P(w_i^{(t)}|z_k)P(d_j^{(t)}|z_k)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**M（maximize）步：**\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\argmax_{\\theta} J(\\theta,Q^{(t)}(z))\n",
    "$$\n",
    "\n",
    "又因为参数满足如下约束条件：\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "    & \\sum_{k=1}^{K} P(z_k) = 1 \\\\\n",
    "    & \\sum_{i=1}^{M} P(w_i|z_k) = 1, k = 1, 2, ..., K \\\\\n",
    "    & \\sum_{j=1}^{N} P(d_j|z_k) = 1, k = 1, 2, ..., K\n",
    "    \\\\ \\\\\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "据此构建Lagrange函数，求解带有约束的优化问题，\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "    \\\\\n",
    "    \\Lambda &= J(\\theta,P(z)) + \\lambda \\left(1 - \\sum_{k=1}^{K} P(z_k)\\right) + \\sum_{k=1}^{K} \\tau_k \\left(1 - \\sum_{i=1}^{M} P(w_i|z_k)\\right) + \\sum_{k=1}^{K} \\rho_k \\left(1 - \\sum_{j=1}^{N} P(d_j|z_k)\\right)\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "解得：\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "    & P(z_k) = \\frac{\\sum_{i=1}^{M}\\sum_{j=1}^{N} n(w_i, d_j)P(z_k|w_i, d_j)}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} n(w_i, d_j)}\\\\\n",
    "    & P(w_i|z_k) = \\frac{\\sum_{j=1}^{N} n(w_i, d_j) P(z_k|w_i, d_j)}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} n(w_i, d_j) P(z_k|w_i, d_j)}\\\\\n",
    "    & P(d_j|z_k) = \\frac{\\sum_{i=1}^{M} n(w_i, d_j) P(z_k|w_i, d_j)}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} n(w_i, d_j) P(z_k|w_i, d_j)}\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "\\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 新闻爬取\n",
    "从[交大新闻网主页新闻栏目](http://news.xjtu.edu.cn/zyxw.htm)爬取最新的100条新闻，编程实现概率潜在语义分析的生成模型或共现模型，并输出不同的话题数下各个话题的高频词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）抓取新闻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "class XJTU_News():\n",
    "    def __init__(self, url):\n",
    "        self.current_url = url  # 主url可以和path拼接\n",
    "        self.cookies = {\"_ga\": \"GA1.3.1733503684.1647506450\"}\n",
    "        self.news_urls = []\n",
    "        self.content = pd.DataFrame(\n",
    "            columns=[\"title\", \"date\", \"content\", \"source\", \"writer\"])\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        response = requests.get(url, cookies=self.cookies)\n",
    "        response.encoding = 'UTF-8-SIG'\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        return soup\n",
    "\n",
    "    def get_news_list(self, path):\n",
    "        self.current_url = parse.urljoin(self.current_url, path)\n",
    "        soup = self.get_soup(self.current_url)\n",
    "        self.news_urls.extend([parse.urljoin(self.current_url, object[\"href\"])\n",
    "                              for object in soup.find_all(\"a\", class_=\"bt\")])\n",
    "        next_page_path = soup.find(\n",
    "            \"span\", class_=\"p_next p_fun\").next_element[\"href\"]\n",
    "        return(next_page_path)\n",
    "\n",
    "    def get_news_lists(self, number):\n",
    "        next_page_path = \"\"\n",
    "        while(len(self.news_urls) < number):\n",
    "            next_page_path = self.get_news_list(next_page_path)\n",
    "\n",
    "    def get_content(self):\n",
    "        for url in self.news_urls:\n",
    "            soup = self.get_soup(url)\n",
    "            title = soup.title.string.split(\"-西安交通大学\")[0]\n",
    "            try:\n",
    "                content = soup.find(\"div\", id=\"vsb_content_2\").text.strip()\n",
    "            except:\n",
    "                content = None # 有的新闻是视频，所以没有content正文\n",
    "                print(url)\n",
    "            writer = soup.find(\"div\", class_=\"zdf clearfix\").text.strip()\n",
    "            source = None\n",
    "            date = None\n",
    "            for temp in soup.find(\"div\", class_=\"shfffff\").contents:\n",
    "                if \"来源\" in temp.text:\n",
    "                    source = temp.text.split(\"：\")[-1].strip()\n",
    "                elif \"日期\" in temp.text:\n",
    "                    date = temp.text.split(\"：\")[-1].strip()\n",
    "                else:\n",
    "                    continue\n",
    "            self.content = self.content.append(\n",
    "                {\"title\": title, \"date\": date, \"content\": content, \"source\": source, \"writer\": writer},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = XJTU_News(url=\"http://news.xjtu.edu.cn/zyxw.htm\")\n",
    "main.get_news_lists(110)\n",
    "main.get_content()\n",
    "main.content.to_csv(\"result.csv\",index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）分词及数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"result.csv\")\n",
    "data[\"text\"] = data[\"title\"]+data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文停用词表：https://github.com/goto456/stopwords\n",
    "stopwords = []\n",
    "f = open(\"cn_stopwords.txt\", \"r\",encoding='utf-8')\n",
    "line = f.readline() # 读取第一行\n",
    "with open(\"cn_stopwords.txt\", \"r\",encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        stopwords.append(line[:-1]) # 列表增加\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"content\"].notna()][:100] # 删除正文为空的数据\n",
    "words=[]\n",
    "for i in range(data.shape[0]):\n",
    "    news = ' '.join(jieba.cut(data.iloc[i][\"content\"]))\n",
    "    words.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# construct co-occurance matrix\n",
    "count_model = CountVectorizer(max_features=2000,max_df=0.5,stop_words=stopwords)\n",
    "word_vector = count_model.fit_transform(words).todense().T      # co-occurance matrix\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）潜在语义分析————共现模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class pLSA():\n",
    "    def __init__(self,step,topic_n,word_vector):\n",
    "        self.step = step #最大步数\n",
    "        self.K = topic_n #话题数量\n",
    "        self.words = word_vector #词向量\n",
    "        self.M,self.N = word_vector.shape #M是词向量长度 ,N是文本数\n",
    "        self.p_w_z = np.random.rand(self.K, self.M)   # p(w|z)\n",
    "        self.p_z_d = np.random.rand(self.N, self.K)   # p(z|d)\n",
    "        self.p_z_wd = np.zeros((self.N, self.M, self.K))   # p(z|w,d)\n",
    "        '''\n",
    "        References\n",
    "        ----------\n",
    "        [1] \"Bayesian Reasoning and Machine Learning\", David Barber (Cambridge\n",
    "        Press, 2012).\n",
    "        [2] plsa.PyPI https://github.com/yedivanseven/PLSA\n",
    "        '''\n",
    "    def E_step(self):\n",
    "        for j in range(self.N):\n",
    "            for i in range(self.M):\n",
    "                temp = np.zeros((self.K))\n",
    "                for k in range(self.K):\n",
    "                    temp[k] = self.p_w_z[k, i] * self.p_z_d[j, k]\n",
    "                self.p_z_wd[j,i] = temp / np.sum(temp)\n",
    "    def M_step(self):\n",
    "        ## p(w|z)\n",
    "        for k in range(self.K):\n",
    "            temp = np.zeros((self.M))\n",
    "            for i in range(self.M):\n",
    "                for j in range(self.N):\n",
    "                    temp[i] += word_vector[i, j] * self.p_z_wd[j, i, k]\n",
    "            self.p_w_z[k] = temp / np.sum(temp)\n",
    "        \n",
    "        ## p(z|d)\n",
    "        for j in range(self.N):\n",
    "            for k in range(self.K):\n",
    "                temp = 0\n",
    "                for i in range(self.M):\n",
    "                    temp += word_vector[i, j] * self.p_z_wd[j, i, k]\n",
    "                self.p_z_d[j, k] = temp / np.sum(word_vector[[j]])\n",
    "    \n",
    "    def fit(self):\n",
    "        for _ in range(self.step):\n",
    "            self.E_step()\n",
    "            self.M_step()\n",
    "        return self.p_w_z, self.p_z_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_n = 3\n",
    "model = pLSA(step = 10,topic_n = topic_n,word_vector = word_vector)\n",
    "p_w_z, p_z_d = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题1：{'青年': 0.01862717415293134, '研究': 0.01543787285185595, '青春': 0.009702259875018139, '时代': 0.009210889165625185, '交大': 0.008298109863527114, '团队': 0.0065938244773482285, '科技': 0.006312667471596037, '表示': 0.006157305897420258, '共青团': 0.00607685885300087, '平台': 0.005269242534540203}\n",
      "\n",
      "主题2：{'学生': 0.01779301337160576, '习近平': 0.0142531717358071, '总书记': 0.014039085573739788, '西迁': 0.013307092219758378, '培养': 0.013076798465364066, '教学': 0.010572538240392362, '课程': 0.009712016007890774, '时代': 0.008596336032387243, '青年': 0.0072026353658971995, '教育': 0.007145105821298227}\n",
      "\n",
      "主题3：{'学生': 0.012331557568073697, '就业': 0.008983833242868363, '活动': 0.008219019650107387, '服务': 0.007425761075039543, '教育': 0.007236010927243649, '体育': 0.007005019234515966, '学科': 0.006794192250061121, '开展': 0.0057322580687101474, '推进': 0.005671051550816049, '加强': 0.005637810487683335}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zjchenb139\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dict_ = count_model.get_feature_names()\n",
    "topic_words = []\n",
    "for k in range(topic_n):\n",
    "    topic_ = np.argsort(-p_w_z[k, :])[:10]\n",
    "    topic_composition = {dict_[i]:p_w_z[k, i] for i in topic_}\n",
    "    print(\"主题{k}：{topic_composition}\\n\".format(k = k+1,topic_composition = topic_composition))\n",
    "    topic_words.append(topic_composition)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb4d4f6bcbd50367daff873984b498aada7467dc07c502b0dfeb9e8751449c28"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
