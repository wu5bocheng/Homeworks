{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS (Alternating Least Squares)\n",
    "\n",
    "* used to solve non-convex optimizations\n",
    "* a matrix factorization algorithm, built for a larges-scale collaborative filtering problems\n",
    "* can be runned in a parallel fashion\n",
    "* solve scalability and sparseness of the ratings data\n",
    "\n",
    "Target function：\n",
    "$$\n",
    "\\min_{p^*,q^*} \\sum_{(i, u)}\\left(r_{u i}-\\mathbf{q}_i^T \\mathbf{p}_u\\right)^2+\\lambda\\left(\\left\\|\\mathbf{q}_i\\right\\|^2+\\left\\|\\mathbf{p}_u\\right\\|^2\\right)\n",
    "$$\n",
    "\n",
    "> [1] [Kevin Liao, 2018 11, Prototyping a Recommender System Step by Step Part 2: Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering](https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1)\n",
    ">\n",
    ">ALS in spark has hyper-params, of which the most important, use grid search CV and RMSE to choose hyper-params: \n",
    ">* maxIter: the maximum number of iterations to run (defaults to 10)\n",
    ">* rank: the number of latent factors in the model (defaults to 10)\n",
    ">* regParam: the regularization parameter in ALS (defaults to 1.0) #正则化参数，即lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_in_movie_index\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# read_data\n",
    "data = pd.read_csv(\"combined_data_1.txt\",sep=\",\",header=None,names = ['customer_id', 'rating'], usecols = [0,1])\n",
    "df_movie = data[pd.isnull(data[\"rating\"])]\n",
    "data[\"movie_id\"] = df_movie[\"customer_id\"].apply(lambda x: x[:-1])\n",
    "data[\"movie_id\"].fillna(method =\"ffill\", inplace = True)\n",
    "data = data[data[\"rating\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2] [jamenlong, 2017 11, Recommendation Engines Using ALS in PySpark (MovieLens Dataset)](https://www.youtube.com/watch?v=FgGjc5oabrA&ab_channel=jamenlong1)\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of\n",
    "    validation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    \n",
    "    validation_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    \n",
    "    maxIter: int, max number of learning iterations\n",
    "    \n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",nonnegative=True).setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Movie Rate\").getOrCreate()\n",
    "(training,test) = spark.createDataFrame(data).randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions:\n",
    "1. predicted ratings is not between 1-5, sometines higher than 5 and lower than 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5679f9553f4f00225f3be8426420a77b8253aec81828e496ef8a4337472ea9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
